{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils import get_extended_attention_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the Feature Embedding Layer. This layer returns three embedding vectors: diagnosis, treatment and visit. It returns mask embedding as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbedder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        \"\"\"Args:\n",
    "        vocab_sizes: A dictionary of vocabularize sizes for each feature. (e.g.{'dx_ints': 1001, 'proc_ints': 1001, 'lab_ints': 1001})\n",
    "        feature_keys: A list of feature names you want to use.\n",
    "        embedding_size: The dimension size of the feature representation vector.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = {}\n",
    "        self.feature_keys = args.feature_keys\n",
    "\n",
    "        self.dx_embeddings = nn.Embedding(args.vocab_sizes['dx_ints'] + 1, args.hidden_size,\n",
    "                                          padding_idx=args.vocab_sizes['dx_ints'])\n",
    "        self.proc_embeddings = nn.Embedding(args.vocab_sizes['proc_ints'] + 1, args.hidden_size,\n",
    "                                            padding_idx=args.vocab_sizes['proc_ints'])\n",
    "        self.visit_embeddings = nn.Embedding(1, args.hidden_size)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(args.hidden_size)\n",
    "        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        embeddings: A dictionary of dense representation Tensors for each feature.\n",
    "        masks: A dictionary of dense float32 Tensors for each feature, that will be used as a mask in the downstream tasks.\"\"\"\n",
    "\n",
    "        batch_size = features[self.feature_keys[0]].shape[0]\n",
    "        embeddings = {}\n",
    "        masks = {}\n",
    "\n",
    "        embeddings['dx_ints'] = self.dx_embeddings(features['dx_ints']) # pass diagnoses ints to embedding layer\n",
    "        embeddings['proc_ints'] = self.proc_embeddings(features['proc_ints']) # pass treatment ints to embedding layer\n",
    "        device = features['dx_ints'].device\n",
    "\n",
    "        embeddings['visit'] = self.visit_embeddings(torch.tensor([0]).to(device))\n",
    "        embeddings['visit'] = embeddings['visit'].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        masks['visit'] = torch.ones(batch_size, 1).to(device)\n",
    "        for name, embedding in embeddings.items():\n",
    "            embeddings[name] = self.layernorm(embedding)\n",
    "            embeddings[name] = self.dropout(embeddings[name])\n",
    "\n",
    "        return embeddings, masks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the script defines self-attention layer. According to the paper, GCT model uses a single-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args, stack_idx):\n",
    "        super().__init__()\n",
    "        self.stack_idx = stack_idx\n",
    "        self.num_attention_heads = args.num_heads\n",
    "        self.attention_head_size = int(args.hidden_size / args.num_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size # ~ hidden size\n",
    "\n",
    "        self.query = nn.Linear(args.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(args.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(args.hidden_size, self.all_head_size)\n",
    "        # experiment with dropout after completion\n",
    "        # self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        \"\"\" This method reshapes the input tensor for the self-attention calculation, which involves taking the dot product between the query and key vectors, scaled by the square root of the attention head size. \"\"\"\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, guide_mask=None, prior=None, output_attentions=True):\n",
    "        if self.stack_idx == 0 and prior is not None:\n",
    "            \"\"\" If prior probabilities are given and this is the first layer of the model (stack_idx == 0), \n",
    "            the prior probabilities are used instead of computing new attention probabilities. \"\"\"\n",
    "            attention_probs = prior[:, None, :, :].expand(-1, self.num_attention_heads, -1, -1)\n",
    "        else:\n",
    "            # computes key and query\n",
    "            mixed_query_layer = self.query(hidden_states)\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            # transpose for calculation\n",
    "            query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "            key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "            # take dot product between query and key to get raw attention scores\n",
    "            attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "            attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "            if attention_mask is not None:\n",
    "                attention_scores = attention_scores + attention_mask # add attention mask\n",
    "            attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer) # compute values\n",
    "\n",
    "        # dropping out entire tokens to attend to; extra experiment\n",
    "        # attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # get context vector\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ```SelfOutput``` represents the output layer in a self-attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOutput(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(args.hidden_size, args.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(args.hidden_size)\n",
    "        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"input: \n",
    "        the output of the self-attention calculation (hidden_states), the original input tensor (input_tensor)\n",
    "        Here, the input tensor is used to add a residual connection to the output of the self-attention calculation. \"\"\"\n",
    "        hidden_states = self.activation(self.dense(hidden_states))\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module connects SelfAttention and SelfOutput, which are defined right above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args, stack_idx):\n",
    "        super().__init__()\n",
    "        self.self_attention = SelfAttention(args, stack_idx)\n",
    "        self.self_output = SelfOutput(args)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, guide_mask=None, prior=None, output_attentions=True):\n",
    "        self_attention_outputs = self.self_attention(hidden_states, attention_mask, guide_mask, prior,\n",
    "                                                     output_attentions) # returns attention output\n",
    "        attention_output = self.self_output(self_attention_outputs[0], hidden_states) # adds the original hidden states as a residual connection, and normalizes the result\n",
    "        outputs = (attention_output,) + self_attention_outputs[1:]\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Intermediate Layer applies transformation to a higher dimensional space and non-linearity by ReLU.\n",
    "The original paper did not use this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateLayer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(args.hidden_size, args.intermediate_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, ```OutputLayer``` gets intermediate output of the block as an input and produces the final output of the block by applying transformation back to ```hidden_size```, LayerNormalization, Dropout and ReLU.\n",
    "The original paper did not use this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(args.intermediate_size, args.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(args.hidden_size)\n",
    "        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.activation(self.dense(hidden_states))\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module defines GCT Layer that has Attention Layer only. If we uncomment IntermediateLayer and OutputLayer that are defined right above, then GCTLayer consists of three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCTLayer(nn.Module):\n",
    "    def __init__(self, args, stack_idx):\n",
    "        super().__init__()\n",
    "        self.attention = Attention(args, stack_idx)\n",
    "        # self.intermediate = IntermediateLayer(args)\n",
    "        # self.output = OutputLayer(args)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, guide_mask=None, prior=None, output_attentions=True):\n",
    "        self_attention_outputs = self.attention(hidden_states, attention_mask, guide_mask, prior, output_attentions)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]\n",
    "\n",
    "        # intermediate_output = self.intermediate(attention_output)\n",
    "        # layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "        # outputs = (layer_output,) + outputs\n",
    "        outputs = (attention_output,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooler(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(args.hidden_size, args.hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0] # extracts the first token tensor \n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code integrates all layers and build the main model, GCT. There could be prior knowledge about the relationships between the diagnosis codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionalTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      feature_keys: A list of feature names you want to use. (e.g. ['dx_ints,'proc_ints', 'lab_ints'])\n",
    "      max_num_codes: The maximum number of how many feature there can be inside\n",
    "        a single visit, per feature. For example, if this is set to 50, then we\n",
    "        are assuming there can be up to 50 diagnosis codes, 50 treatment codes,\n",
    "        and 50 lab codes. This will be used for creating the prior matrix.\n",
    "      prior_scalar: A float value between 0.0 and 1.0 to be used to hard-code the diagnoal elements of the prior matrix.\n",
    "      reg_coef: A coefficient to decide the KL regularization balance when training GCT.\n",
    "      batch_size: Batch size.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super(GraphConvolutionalTransformer, self).__init__()\n",
    "        self.num_labels = args.num_labels\n",
    "        self.label_key = args.label_key\n",
    "        self.reg_coef = args.reg_coef\n",
    "        self.use_guide = args.use_guide\n",
    "        self.use_prior = args.use_prior\n",
    "        self.prior_scalar = args.prior_scalar\n",
    "        self.batch_size = args.batch_size\n",
    "        self.num_stacks = args.num_stacks\n",
    "        self.max_num_codes = args.max_num_codes\n",
    "        self.output_attentions = args.output_attentions\n",
    "        self.output_hidden_states = args.output_hidden_states\n",
    "        self.feature_keys = args.feature_keys\n",
    "        self.layers = nn.ModuleList([GCTLayer(args, i) for i in range(args.num_stacks)])\n",
    "        self.embeddings = FeatureEmbedder(args)\n",
    "        self.pooler = Pooler(args)\n",
    "\n",
    "        # self.dropout = nn.Dropout(args.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(args.hidden_size, args.num_labels)\n",
    "\n",
    "    def create_matrix_vdp(self, features, masks, priors):\n",
    "        \"\"\"vdp indicate your features are diagnosis codes and treatment codes only.\n",
    "        This function creates the guide matrix and the prior matrix when visits include diagnosis codes, treatment codes, but not lab codes.\n",
    "\n",
    "        Args:\n",
    "            features: A dictionary of SparseTensors for each feature.\n",
    "            mask: 3D float Tensor of size (batch_size, num_features, 1). This holds binary values to indicate the parts that are padded.\n",
    "        Returns:\n",
    "            guide: The guide matrix.\n",
    "            prior_guide: The conditional probablity matrix.\n",
    "        \"\"\"\n",
    "        batch_size = features['dx_ints'].shape[0]\n",
    "        device = features['dx_ints'].device\n",
    "        num_dx_ids = self.max_num_codes if self.use_prior else features['dx_ints'].shape[-1]\n",
    "        num_proc_ids = self.max_num_codes if self.use_prior else features['proc_ints'].shape[-1]\n",
    "        num_codes = 1 + num_dx_ids + num_proc_ids\n",
    "\n",
    "        guide = None\n",
    "        prior_guide = None\n",
    "\n",
    "        # If use_guide is True, it creates a guide matrix that encodes the dependencies between diagnosis codes and treatment codes.\n",
    "        if self.use_guide:\n",
    "            row0 = torch.cat([torch.zeros([1, 1]), torch.ones([1, num_dx_ids]), torch.zeros([1, num_proc_ids])], axis=1)\n",
    "            row1 = torch.cat([torch.zeros([num_dx_ids, num_dx_ids + 1]), torch.ones([num_dx_ids, num_proc_ids])],\n",
    "                             axis=1)\n",
    "            row2 = torch.zeros([num_proc_ids, num_codes])\n",
    "\n",
    "            guide = torch.cat([row0, row1, row2], axis=0)\n",
    "            guide = guide + guide.t()\n",
    "            guide = guide.to(device)\n",
    "\n",
    "            guide = guide.unsqueeze(0)\n",
    "            guide = guide.expand(batch_size, -1, -1)\n",
    "            guide = (guide * masks.unsqueeze(-1) * masks.unsqueeze(1) + torch.eye(num_codes).to(device).unsqueeze(0))\n",
    "        \n",
    "        # If use_prior is True, it creates a prior guide matrix based on the prior knowledge provided.\n",
    "        if self.use_prior:\n",
    "            prior_idx = priors['indices'].t()\n",
    "            temp_idx = (prior_idx[:, 0] * 100000 + prior_idx[:, 1] * 1000 + prior_idx[:, 2])\n",
    "            sorted_idx = torch.argsort(temp_idx)\n",
    "            prior_idx = prior_idx[sorted_idx]\n",
    "\n",
    "            prior_idx_shape = [batch_size, self.max_num_codes * 2, self.max_num_codes * 2]\n",
    "            sparse_prior = torch.sparse.FloatTensor(prior_idx.t(), priors['values'], torch.Size(prior_idx_shape))\n",
    "            prior_guide = sparse_prior.to_dense()\n",
    "\n",
    "            visit_guide = torch.tensor([self.prior_scalar] * self.max_num_codes + [0.0] * self.max_num_codes * 1,\n",
    "                                       dtype=torch.float, device=device)\n",
    "            prior_guide = torch.cat([visit_guide.unsqueeze(0).unsqueeze(0).expand(batch_size, -1, -1), prior_guide],\n",
    "                                    axis=1)\n",
    "            visit_guide = torch.cat([torch.tensor([0.0], device=device), visit_guide], axis=0)\n",
    "            prior_guide = torch.cat([visit_guide.unsqueeze(0).unsqueeze(-1).expand(batch_size, -1, -1), prior_guide],\n",
    "                                    axis=2)\n",
    "            prior_guide = (prior_guide * masks.unsqueeze(-1) * masks.unsqueeze(1) + self.prior_scalar * torch.eye(\n",
    "                num_codes, device=device).unsqueeze(0))\n",
    "            degrees = torch.sum(prior_guide, axis=2)\n",
    "            prior_guide = prior_guide / degrees.unsqueeze(-1)\n",
    "\n",
    "        return guide, prior_guide\n",
    "\n",
    "    def get_loss(self, logits, labels, attentions): # computes crossEntropyLoss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if self.use_prior: # If prior is used, we regularize loss function by KL regularization. This prevents the self-attention values becoming too novel from prior knowledge.\n",
    "            kl_terms = []\n",
    "            for i in range(1, self.num_stacks):\n",
    "                log_p = torch.log(attentions[i - 1] + 1e-12)\n",
    "                log_q = torch.log(attentions[i] + 1e-12)\n",
    "                kl_term = attentions[i - 1] * (log_p - log_q)\n",
    "                kl_term = torch.sum(kl_term, axis=-1)\n",
    "                kl_term = torch.mean(kl_term)\n",
    "                kl_terms.append(kl_term)\n",
    "            reg_term = torch.mean(torch.tensor(kl_terms))\n",
    "            loss += self.reg_coef * reg_term\n",
    "        return loss\n",
    "\n",
    "    def forward(self, data, priors_data):\n",
    "        embedding_dict, mask_dict = self.embeddings(data)\n",
    "        mask_dict['dx_ints'] = data['dx_masks']\n",
    "        mask_dict['proc_ints'] = data['proc_masks']\n",
    "\n",
    "        keys = ['visit'] + self.feature_keys\n",
    "        hidden_states = torch.cat([embedding_dict[key] for key in keys], axis=1)\n",
    "        masks = torch.cat([mask_dict[key] for key in keys], axis=1)\n",
    "\n",
    "        guide, prior_guide = self.create_matrix_vdp(data, masks, priors_data)\n",
    "\n",
    "        all_hidden_states = () if self.output_hidden_states else None\n",
    "        all_attentions = () if self.output_attentions else None\n",
    "        # make attention_mask, guide_mask\n",
    "        extended_attention_mask = get_extended_attention_mask(masks)\n",
    "        extended_guide_mask = get_extended_attention_mask(guide) if self.use_guide else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layers):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, extended_attention_mask, extended_guide_mask, prior_guide,\n",
    "                                         self.output_attentions)\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        pooled_output = self.pooler(hidden_states)\n",
    "        # pooled_output = hidden_states[:,0]\n",
    "\n",
    "        # get logits and loss\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = self.get_loss(logits, data[self.label_key], all_attentions)\n",
    "\n",
    "        return tuple(v for v in [loss, logits, all_hidden_states, all_attentions] if v is not None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm, trange\n",
    "from utils import *\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load all processed data that are going to be used for train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "data_dir = \"data\"\n",
    "fold_path = os.path.join(data_dir, 'fold_{}'.format(fold))\n",
    "cached_path = os.path.join(fold_path, 'cached')\n",
    "\n",
    "train_dataset = torch.load(os.path.join(cached_path, 'train_dataset.pt'))\n",
    "eval_dataset = torch.load(os.path.join(cached_path, 'valid_dataset.pt'))\n",
    "test_dataset = torch.load(os.path.join(cached_path, 'test_dataset.pt'))\n",
    "\n",
    "train_priors = torch.load(os.path.join(cached_path, 'train_priors.pt'))\n",
    "eval_priors = torch.load(os.path.join(cached_path, 'valid_priors.pt'))\n",
    "test_priors = torch.load(os.path.join(cached_path, 'test_priors.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define ```prediction_loop()```, which returns a ```metrics``` as a dictionary.\n",
    "The dictionary contains ```eval_loss``` key, which is the mean loss across all batches.\n",
    "There are other keys prefixed with 'eval_' for the other evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_loop(args, model, dataloader, priors_datalaoder, description='Evaluating'):\n",
    "    batch_size = dataloader.batch_size\n",
    "    eval_losses = []\n",
    "    preds = None\n",
    "    label_ids = None\n",
    "    model.eval() # set model mode as evaluation mode\n",
    "\n",
    "    for data, priors_data in tqdm(zip(dataloader, priors_datalaoder), desc=description): \n",
    "        # Iterates over batches of data from the dataloader and priors_dataloader\n",
    "        data, priors_data = prepare_data(data, priors_data, args.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(data, priors_data)\n",
    "            loss = outputs[0].mean().item()\n",
    "            logits = outputs[1]\n",
    "\n",
    "        labels = data[args.label_key]\n",
    "\n",
    "        batch_size = data[list(data.keys())[0]].shape[0]\n",
    "        eval_losses.extend([loss] * batch_size)\n",
    "        preds = logits if preds is None else nested_concat(preds, logits, dim=0)\n",
    "        label_ids = labels if label_ids is None else nested_concat(label_ids, labels, dim=0)\n",
    "\n",
    "    if preds is not None:\n",
    "        preds = nested_numpify(preds)\n",
    "    if label_ids is not None:\n",
    "        label_ids = nested_numpify(label_ids)\n",
    "    metrics = compute_metrics(preds, label_ids)\n",
    "\n",
    "    metrics['eval_loss'] = np.mean(eval_losses) # update loss value as mean of loss per a batch\n",
    "\n",
    "    for key in list(metrics.keys()):\n",
    "        if not key.startswith('eval_'):\n",
    "            metrics['eval_{}'.format(key)] = metrics.pop(key)\n",
    "\n",
    "    return metrics # return metrics dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will test for readmission prediction on eICU using the hyperparameters stated in the paper Table 6.\n",
    "\n",
    "> **readmission prediction on eICU**\n",
    ">\n",
    "- Learning Rate: 0.00022\n",
    ">\n",
    "- MLP dropoutrate: 0.08\n",
    ">\n",
    "- Post-MLP dropout rate: 0.024\n",
    ">\n",
    "- Regularization coef.: 0.1\n",
    "\n",
    "> **evaluation**\n",
    ">\n",
    "- Validation AUCPR: 0.5313 (0.0124)\n",
    ">\n",
    "- Test AUCPR: 0.5244 (0.0142)\n",
    ">\n",
    "- Validation AUROC: 0.7525 (0.0128)\n",
    ">\n",
    "- Test AUROC: 0.7502 (0.0114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.num_labels = 2 # default\n",
    "args.label_key = 'readmission'\n",
    "args.reg_coef = 0.1\n",
    "args.use_guide = True\n",
    "args.use_prior = True\n",
    "args.prior_scalar = 0.5 # default\n",
    "args.batch_size = 32\n",
    "args.num_stacks = 2\n",
    "args.max_num_codes = 50 # default\n",
    "args.output_attentions = True\n",
    "args.output_hidden_states = True\n",
    "args.feature_keys = ['dx_ints', 'proc_ints'] # default\n",
    "args.hidden_size = 128 # default\n",
    "args.num_heads = 1 # default\n",
    "args.hidden_dropout_prob = 0.08\n",
    "args.vocab_sizes = {'dx_ints':3249, 'proc_ints':2210} # default\n",
    "args.learning_rate = 0.00022\n",
    "args.max_steps=1000000\n",
    "args.output_dir=f'eicu_output/model_{args.learning_rate}_{args.hidden_dropout_prob}_{args.label_key}'\n",
    "args.do_train = True\n",
    "args.do_eval = True\n",
    "args.do_test = True\n",
    "args.warmup = 0.05 # default\n",
    "args.intermediate_size = 256 # default\n",
    "args.eps = 1e-8 # default\n",
    "args.max_grad_norm = 1.0 # default\n",
    "args.eval_batch_size = 32  \n",
    "args.logging_steps=100 # default\n",
    "args.num_train_epochs = 0 # default\n",
    "args.seed = 42 # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(model): # This prints out the setting information of model parameters\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        shape = param.shape\n",
    "        param_size = 1\n",
    "        for dim in shape:\n",
    "            param_size *= dim\n",
    "        print(name, shape, param_size)\n",
    "        total_params += param_size\n",
    "    print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the log data\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "logging_dir = os.path.join(args.output_dir, 'logging')\n",
    "if not os.path.exists(logging_dir):\n",
    "    os.makedirs(logging_dir)\n",
    "tb_writer = SummaryWriter(log_dir=logging_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell output was a verbose logging, so removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset handling\n",
    "train_priors_dataset = eICUDataset(train_priors)\n",
    "eval_priors_dataset = eICUDataset(eval_priors)\n",
    "test_priors_dataset = eICUDataset(test_priors)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=args.batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size)\n",
    "\n",
    "train_priors_dataloader = DataLoader(train_priors_dataset, batch_size=args.batch_size, collate_fn=priors_collate_fn)\n",
    "eval_priors_dataloader = DataLoader(eval_priors_dataset, batch_size=args.batch_size, collate_fn=priors_collate_fn)\n",
    "test_priors_dataloader = DataLoader(test_priors_dataset, batch_size=args.batch_size, collate_fn=priors_collate_fn)\n",
    "\n",
    "# Check GPU Resource\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if args.device.type == 'cuda':\n",
    "    torch.cuda.set_device(args.device)\n",
    "    logger.info('***** Using CUDA device *****')\n",
    "\n",
    "if args.do_train:\n",
    "    model = GraphConvolutionalTransformer(args)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    if args.max_steps > 0:\n",
    "        max_steps = args.max_steps\n",
    "        num_train_epochs = max_steps // num_update_steps_per_epoch + int(max_steps % num_update_steps_per_epoch > 0)\n",
    "    else:\n",
    "        max_steps = int(num_update_steps_per_epoch * args.num_train_epochs)\n",
    "        num_train_epochs = args.num_train_epochs\n",
    "    num_train_epochs = int(np.ceil(num_train_epochs))\n",
    "\n",
    "    args.eval_steps = num_update_steps_per_epoch // 2\n",
    "\n",
    "    # also try Adamax\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, betas=(0.9,0.999), eps=args.eps)\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=args.learning_rate)\n",
    "    warmup_steps = max_steps // (1 / args.warmup)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "    # if tb_writer:\n",
    "    #     tb_writer.add_text('args', json.dumps(vars(args), indent=2, sort_keys=True))\n",
    "\n",
    "    logger.info('***** Running Training *****')\n",
    "    logger.info(' Num examples = {}'.format(len(train_dataloader.dataset)))\n",
    "    logger.info(' Num epochs = {}'.format(num_train_epochs))\n",
    "    logger.info(' Train batch size = {}'.format(args.batch_size))\n",
    "    logger.info(' Total optimization steps = {}'.format(max_steps))\n",
    "\n",
    "    epochs_trained = 0\n",
    "    global_step = 0\n",
    "    tr_loss = torch.tensor(0.0).to(args.device)\n",
    "    logging_loss_scalar = 0.0\n",
    "    model.zero_grad()\n",
    "\n",
    "    train_pbar = trange(epochs_trained, num_train_epochs, desc='Epoch')\n",
    "    for epoch in range(epochs_trained, num_train_epochs): # Iterate over Epoch\n",
    "        epoch_pbar = tqdm(train_dataloader, desc='Iteration')\n",
    "        for data, priors_data in zip(train_dataloader, train_priors_dataloader):\n",
    "            model.train()\n",
    "            data, priors_data = prepare_data(data, priors_data, args.device)\n",
    "\n",
    "            # [loss, logits, all_hidden_states, all_attentions]\n",
    "            outputs = model(data, priors_data)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.detach()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            if (args.logging_steps > 0 and global_step % args.logging_steps == 0):\n",
    "                logs = {}\n",
    "                tr_loss_scalar = tr_loss.item()\n",
    "                logs['loss'] = (tr_loss_scalar - logging_loss_scalar) / args.logging_steps\n",
    "                logs['learning_rate'] = scheduler.get_last_lr()[0]\n",
    "                logging_loss_scalar = tr_loss_scalar\n",
    "                if tb_writer:\n",
    "                    for k, v in logs.items():\n",
    "                        if isinstance(v, (int, float)):\n",
    "                            tb_writer.add_scalar(k, v, global_step)\n",
    "                    tb_writer.flush()\n",
    "                output = {**logs, **{\"step\": global_step}}\n",
    "                print(output)\n",
    "\n",
    "            if (args.eval_steps > 0 and global_step % args.eval_steps == 0):\n",
    "                metrics = prediction_loop(args, model, eval_dataloader, eval_priors_dataloader)\n",
    "                logger.info('**** Checkpoint Eval Results ****')\n",
    "                for key, value in metrics.items():\n",
    "                    logger.info('{} = {}'.format(key, value))\n",
    "                    tb_writer.add_scalar(key, value, global_step)\n",
    "\n",
    "            epoch_pbar.update(1)\n",
    "            if global_step >= max_steps:\n",
    "                break\n",
    "        epoch_pbar.close()\n",
    "        train_pbar.update(1)\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "\n",
    "    train_pbar.close()\n",
    "    if tb_writer:\n",
    "        tb_writer.close()\n",
    "\n",
    "    logging.info('\\n\\nTraining completed')\n",
    "\n",
    "eval_results = {}\n",
    "### Evaluation\n",
    "if args.do_eval:\n",
    "    logger.info('*** Evaluate ***')\n",
    "    logger.info(' Num examples = {}'.format(len(eval_dataloader.dataset)))\n",
    "    eval_result = prediction_loop(args, model, eval_dataloader, eval_priors_dataloader)\n",
    "    output_eval_file = os.path.join(args.output_dir, 'eval_results.txt')\n",
    "    with open(output_eval_file, 'w') as writer:\n",
    "        logger.info('*** Eval Results ***')\n",
    "        for key, value in eval_result.items():\n",
    "            logger.info(\"{} = {}\".format(key, value))\n",
    "            writer.write('{} = {}'.format(key, value))\n",
    "    eval_results.update(eval_result)\n",
    "\n",
    "### Test\n",
    "if args.do_test:\n",
    "    logging.info('*** Test ***')\n",
    "    # predict\n",
    "    test_result = prediction_loop(args, model, test_dataloader, test_priors_dataloader, description='Testing')\n",
    "    output_test_file = os.path.join(args.output_dir, 'test_results.txt')\n",
    "    with open(output_test_file, 'w') as writer:\n",
    "        logger.info('**** Test results ****')\n",
    "        for key, value in test_result.items():\n",
    "            logger.info('{} = {}'.format(key, value))\n",
    "            writer.write('{} = {}'.format(key, value))\n",
    "    eval_results.update(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > cat test_results.txt\n",
    "# > eval_loss = 3.675219775582708\n",
    "# > eval_AUCPR = 0.4080743460147017\n",
    "# > eval_AUROC = 0.6088238544244021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

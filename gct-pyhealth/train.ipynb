{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Graph Convolution Transformer (GCT) for eICU dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 0: Import libaries, prepare proper parameters**\n",
    "- **[README]:** We provide a set of default parameters for users to run the experiments. Users can also change the parameters to fit their own needs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from gctpyhealth.process_eicu_dataset import get_eicu_datasets\n",
    "from gctpyhealth.utils import *\n",
    "from gctpyhealth.gct import GCT\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchsummary as summary\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:11.493879Z",
     "end_time": "2023-05-02T01:26:11.566883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:11.500680Z",
     "end_time": "2023-05-02T01:26:11.570865Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, prediction_task: str):\n",
    "        if prediction_task == \"expired\":\n",
    "            self.label_key = \"expired\"\n",
    "            self.learning_rate = 0.00011\n",
    "            self.reg_coef = 1.5\n",
    "            self.hidden_dropout = 0.72\n",
    "        elif prediction_task == \"readmission\":\n",
    "            self.label_key = \"readmission\"\n",
    "            self.learning_rate = 0.00022\n",
    "            self.reg_coef = 0.1\n",
    "            self.hidden_dropout = 0.08\n",
    "        else:\n",
    "            raise ValueError(\"Invalid prediction task: {}\".format(prediction_task))\n",
    "\n",
    "        # Training arguments\n",
    "        self.max_steps = 5000  ### for short run # 1000000\n",
    "        self.warmup = 0.05  # default\n",
    "        self.logging_steps = 100  # default\n",
    "        self.num_train_epochs = 1  # default\n",
    "        self.seed = 42  # default\n",
    "\n",
    "        # Model parameters arguments\n",
    "        self.embedding_dim = 128\n",
    "        self.max_num_codes = 50\n",
    "        self.num_stacks = 3\n",
    "        self.batch_size = 32\n",
    "        self.prior_scalar = 0.5\n",
    "        self.num_heads = 1\n",
    "\n",
    "        # save and load the cache/dataset/env path (required)\n",
    "        self.fold = 0\n",
    "        self.data_dir = \"eicu_data\"\n",
    "        self.eicu_csv_dir = \"../eicu_csv\"\n",
    "        # timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        # self.output_dir = \"eicu_output/model_pyhealth_\" + timestamp\n",
    "        self.output_dir = \"eicu_output/model_pyhealth_\" + self.label_key\n",
    "\n",
    "        # save and load the models (optional)\n",
    "        self.save_model = True\n",
    "        self.load_prev_model = False\n",
    "        self.prev_model_path = \"eicu_output/model_pyhealth_\" + self.label_key + \"/model.pt\"\n",
    "\n",
    "\n",
    "args = Args(\"expired\")\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 1: Load dataset**\n",
    "- **[README]:** We call [pyhealth.datasets](https://pyhealth.readthedocs.io/en/latest/api/datasets.html) to process and obtain the dataset.\n",
    "  - `root` is the arguments directing to the data folder.\n",
    "  - `tables` is a list of table names from raw databases, which specifies the information that will be used in building the pipeline. Currently, we provide [MIMIC3Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html), [MIMIC4Dataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC4Dataset.html), [eICUDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.eICUDataset.html), [OMOPDataset](https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.OMOPDataset.html).\n",
    "  - `code_mapping [default: None]` asks a directionary input, specifying the new coding systems for each data table. For example, `{\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})}` means that our pyhealth will automatically change the codings from `NDC` into ATC-3 level for tables if any.\n",
    "  - `dev`: if set `True`, will only load a smaller set of patients.\n",
    "- **[Next Step]:** This `pyhealth.datasets` object will be used in **Step 2**.\n",
    "- **[Advanced Use Case]:** Researchers can use the dict-based output alone `dataset.patients` alone for supporting their own tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:11.508428Z",
     "end_time": "2023-05-02T01:26:11.625553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the log data\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "logging_dir = os.path.join(args.output_dir, 'logging')\n",
    "if not os.path.exists(logging_dir):\n",
    "    os.makedirs(logging_dir)\n",
    "tb_writer = SummaryWriter(log_dir=logging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eICU dataset\n",
      "\n",
      "Statistics of base dataset (dev=True):\n",
      "\t- Dataset: eICUDataset\n",
      "\t- Number of patients: 3671\n",
      "\t- Number of visits: 5000\n",
      "\t- Number of visits per patient: 1.3620\n",
      "\t- Number of events per visit in admissionDx: 2.7186\n",
      "\t- Number of events per visit in diagnosisString: 4.8976\n",
      "\t- Number of events per visit in treatment: 0.0000\n",
      "\n",
      "\n",
      "Statistics of base dataset (dev=True):\n",
      "\t- Dataset: eICUDataset\n",
      "\t- Number of patients: 3671\n",
      "\t- Number of visits: 5000\n",
      "\t- Number of visits per patient: 1.3620\n",
      "\t- Number of events per visit in admissionDx: 2.7186\n",
      "\t- Number of events per visit in diagnosisString: 4.8976\n",
      "\t- Number of events per visit in treatment: 0.0000\n",
      "\n",
      "\n",
      "dataset.patients: patient_id -> <Patient>\n",
      "\n",
      "<Patient>\n",
      "    - visits: visit_id -> <Visit> \n",
      "    - other patient-level info\n",
      "    \n",
      "    <Visit>\n",
      "        - event_list_dict: table_name -> List[Event]\n",
      "        - other visit-level info\n",
      "    \n",
      "        <Event>\n",
      "            - code: str\n",
      "            - other event-level info\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading the eICU dataset\n",
    "from pyhealth.datasets import eICUDataset\n",
    "\n",
    "print('Loading eICU dataset')\n",
    "eicu_ds = eICUDataset(\n",
    "    root=args.eicu_csv_dir,\n",
    "    tables=[\"admissionDx\", \"diagnosisString\", \"treatment\"],\n",
    "    refresh_cache=False,\n",
    "    dev=True\n",
    ")\n",
    "\n",
    "print(eicu_ds.stat())\n",
    "print(eicu_ds.info())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:11.518612Z",
     "end_time": "2023-05-02T01:26:11.939681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 2: Create Dataloader**\n",
    "- **[README]:** We can also load the preprocessed datasets dict from cache and create the dataloader accordingly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:11.929929Z",
     "end_time": "2023-05-02T01:26:14.639990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached data, loading...\n",
      "loading cached data takes: 2.6298274993896484s\n"
     ]
    }
   ],
   "source": [
    "# fetch the datatset from caches\n",
    "datasets, prior_guides = get_eicu_datasets(args.data_dir, args.eicu_csv_dir, fold=args.fold)\n",
    "train_dataset, eval_dataset, test_dataset = datasets\n",
    "train_priors, eval_priors, test_priors = prior_guides\n",
    "train_priors_dataset = eICUPriorDataset(train_priors)\n",
    "eval_priors_dataset = eICUPriorDataset(eval_priors)\n",
    "test_priors_dataset = eICUPriorDataset(test_priors)\n",
    "\n",
    "# prepare data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=args.batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size)\n",
    "\n",
    "train_priors_dataloader = DataLoader(train_priors_dataset,\n",
    "                                     batch_size=args.batch_size, collate_fn=priors_collate_fn)\n",
    "eval_priors_dataloader = DataLoader(eval_priors_dataset,\n",
    "                                    batch_size=args.batch_size, collate_fn=priors_collate_fn)\n",
    "test_priors_dataloader = DataLoader(test_priors_dataset,\n",
    "                                    batch_size=args.batch_size, collate_fn=priors_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/02/2023 01:26:14 - INFO - __main__ - ***** Using CUDA device *****\n"
     ]
    }
   ],
   "source": [
    "# check if gpu/cuda is available\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.set_device(device)\n",
    "    logger.info('***** Using CUDA device *****')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:14.640945Z",
     "end_time": "2023-05-02T01:26:14.644862Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 3: Define ML Model**\n",
    "- **[README]:** We initialize an ML model for the healthcare task by calling [pyhealth.models](https://pyhealth.readthedocs.io/en/latest/api/models.html).\n",
    "- **[Next Step]:** This `pyhealth.models` object will be used in **Step 4**.\n",
    "- **[Other Use Case]:** Our `pyhealth.models` object is as general as any instance from `torch.nn.Module`. Users may use it separately for supporting any other customized pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# from pyhealth.models import Transformer\n",
    "from gctpyhealth.gct import GCT\n",
    "\n",
    "model = GCT(\n",
    "    dataset=eicu_ds,\n",
    "    feature_keys=['conditions_hash',\n",
    "                  'procedures_hash'],\n",
    "    label_key=args.label_key,\n",
    "    mode=\"binary\",\n",
    "    embedding_dim=args.embedding_dim,\n",
    "    max_num_codes=args.max_num_codes,\n",
    "    num_stacks=args.num_stacks,\n",
    "    batch_size=args.batch_size,\n",
    "    reg_coef=args.reg_coef,\n",
    "    prior_scalar=args.prior_scalar,\n",
    "    hidden_dropout=args.hidden_dropout,\n",
    "    num_heads=args.num_heads,\n",
    ")\n",
    "\n",
    "# loading previous checkpoint if available\n",
    "checkpoint = None\n",
    "if args.load_prev_model:\n",
    "    checkpoint = torch.load(args.prev_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:14.644857Z",
     "end_time": "2023-05-02T01:26:14.703894Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 4: Model Training**\n",
    "- **[README]:** We call our [pyhealth.train.Trainer](https://pyhealth.readthedocs.io/en/latest/api/trainer.html) to train the model by giving the `train_loader`, the `val_loader`, val_metric, and specify other arguemnts, such as epochs, optimizer, learning rate, etc. The trainer will automatically save the best model and output the path in the end.\n",
    "- **[Next Step]:** The best model will be used in **Step 5** for evaluation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/02/2023 01:26:14 - INFO - __main__ - ***** Running Training *****\n",
      "05/02/2023 01:26:14 - INFO - __main__ -  Num examples = 32820\n",
      "05/02/2023 01:26:14 - INFO - __main__ -  Num epochs = 5\n",
      "05/02/2023 01:26:14 - INFO - __main__ -  Train batch size = 32\n",
      "05/02/2023 01:26:14 - INFO - __main__ -  Total optimization steps = 5000\n"
     ]
    }
   ],
   "source": [
    "# compute how steps and epoch is required\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "if args.max_steps > 0:\n",
    "    max_steps = args.max_steps\n",
    "    num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "        args.max_steps % num_update_steps_per_epoch > 0)\n",
    "else:\n",
    "    max_steps = int(num_update_steps_per_epoch * args.num_train_epochs)\n",
    "    num_train_epochs = args.num_train_epochs\n",
    "num_train_epochs = int(np.ceil(num_train_epochs))\n",
    "\n",
    "args.eval_steps = num_update_steps_per_epoch // 2\n",
    "\n",
    "# prepare optimizer, scheduler\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=args.learning_rate)\n",
    "warmup_steps = max_steps // (1 / args.warmup)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "logger.info('***** Running Training *****')\n",
    "logger.info(' Num examples = {}'.format(len(train_dataloader.dataset)))\n",
    "logger.info(' Num epochs = {}'.format(num_train_epochs))\n",
    "logger.info(' Train batch size = {}'.format(args.batch_size))\n",
    "logger.info(' Total optimization steps = {}'.format(max_steps))\n",
    "\n",
    "epochs_trained = 0\n",
    "global_step = 0\n",
    "tr_loss = torch.tensor(0.0).to(device)\n",
    "logging_loss_scalar = 0.0\n",
    "model.zero_grad()\n",
    "\n",
    "# check if we have previous checkpoint\n",
    "if args.load_prev_model and checkpoint is not None:\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epochs_trained = checkpoint['epochs_trained']\n",
    "    global_step = checkpoint['global_step']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:14.693527Z",
     "end_time": "2023-05-02T01:26:14.704349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 1/1026 [00:00<00:12, 79.56it/s]\n",
      "Epoch:  20%|██        | 1/5 [00:00<00:00, 70.51it/s]\n",
      "05/02/2023 01:26:14 - INFO - root - \n",
      "\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "training_outputs = None\n",
    "train_pbar = trange(epochs_trained, num_train_epochs, desc='Epoch')\n",
    "for epoch in range(epochs_trained, num_train_epochs):\n",
    "    epoch_pbar = tqdm(train_dataloader, desc='Iteration')\n",
    "    for data, priors_data in zip(train_dataloader, train_priors_dataloader):\n",
    "        model.train()\n",
    "        data, priors_data = prepare_data(data, priors_data, device)\n",
    "\n",
    "        # [loss, logits, all_hidden_states, all_attentions]\n",
    "        training_outputs = model(data, priors_data)\n",
    "        loss = training_outputs['loss']\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.detach()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        # update the global step\n",
    "        global_step += 1\n",
    "\n",
    "        # print out the training results\n",
    "        if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "            logs = {}\n",
    "            tr_loss_scalar = tr_loss.item()\n",
    "            logs['loss'] = (tr_loss_scalar - logging_loss_scalar) / args.logging_steps\n",
    "            logs['learning_rate'] = scheduler.get_last_lr()[0]\n",
    "            logging_loss_scalar = tr_loss_scalar\n",
    "            if tb_writer:\n",
    "                for k, v in logs.items():\n",
    "                    if isinstance(v, (int, float)):\n",
    "                        tb_writer.add_scalar(k, v, global_step)\n",
    "                tb_writer.flush()\n",
    "            output = {**logs, **{\"step\": global_step}}\n",
    "            print(output)\n",
    "\n",
    "        # print out the evaluation results\n",
    "        if args.eval_steps > 0 and global_step % args.eval_steps == 0:\n",
    "            metrics = prediction_loop(device, args.label_key,\n",
    "                                      model, eval_dataloader, eval_priors_dataloader)\n",
    "            logger.info('**** Checkpoint Eval Results ****')\n",
    "            for key, value in metrics.items():\n",
    "                logger.info('{} = {}'.format(key, value))\n",
    "                tb_writer.add_scalar(key, value, global_step)\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "\n",
    "    epoch_pbar.close()\n",
    "    train_pbar.update(1)\n",
    "    if global_step >= max_steps:\n",
    "        break\n",
    "\n",
    "train_pbar.close()\n",
    "if tb_writer:\n",
    "    tb_writer.close()\n",
    "\n",
    "logging.info('\\n\\nTraining completed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:14.706664Z",
     "end_time": "2023-05-02T01:26:14.791038Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 5: Evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/02/2023 01:26:14 - INFO - __main__ - *** Evaluate ***\n",
      "05/02/2023 01:26:14 - INFO - __main__ -  Num examples = 4103\n",
      "Evaluating: 129it [00:00, 282.63it/s]\n",
      "05/02/2023 01:26:15 - INFO - __main__ - *** Eval results @ steps:5005 ***\n",
      "\n",
      "05/02/2023 01:26:15 - INFO - __main__ - eval_loss = 2.6260850043114354\n",
      "\n",
      "05/02/2023 01:26:15 - INFO - __main__ - eval_AUCPR = 0.5114991306163474\n",
      "\n",
      "05/02/2023 01:26:15 - INFO - __main__ - eval_AUROC = 0.6680150251867292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "eval_results = {}\n",
    "\n",
    "logger.info('*** Evaluate ***')\n",
    "logger.info(' Num examples = {}'.format(len(eval_dataloader.dataset)))\n",
    "eval_result = prediction_loop(device, args.label_key, model, eval_dataloader, eval_priors_dataloader)\n",
    "output_eval_file = os.path.join(args.output_dir, 'eval_results.txt')\n",
    "\n",
    "with open(output_eval_file, 'a') as writer:\n",
    "    logger.info('*** Eval results @ steps:{} ***\\n'.format(global_step))\n",
    "    writer.write('*** Eval results @ steps:{} ***\\n'.format(global_step))\n",
    "    for key, value in eval_result.items():\n",
    "        logger.info('{} = {}\\n'.format(key, value))\n",
    "        writer.write('{} = {}\\n'.format(key, value))\n",
    "eval_results.update(eval_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:14.750351Z",
     "end_time": "2023-05-02T01:26:15.247734Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 6: Inference**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/02/2023 01:26:15 - INFO - root - *** Test ***\n",
      "Testing: 129it [00:00, 311.98it/s]\n",
      "05/02/2023 01:26:15 - INFO - __main__ - *** Test results @ steps:5005 ***\n",
      "\n",
      "05/02/2023 01:26:15 - INFO - __main__ - eval_loss = 2.6158219755146233\n",
      "\n",
      "05/02/2023 01:26:15 - INFO - __main__ - eval_AUCPR = 0.5238963374929265\n",
      "\n",
      "05/02/2023 01:26:15 - INFO - __main__ - eval_AUROC = 0.6612314841190714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test and predict\n",
    "logging.info('*** Test ***')\n",
    "test_result = prediction_loop(device, args.label_key, model, test_dataloader, test_priors_dataloader,\n",
    "                              description='Testing')\n",
    "output_test_file = os.path.join(args.output_dir, 'test_results.txt')\n",
    "with open(output_test_file, 'a') as writer:\n",
    "    logger.info('*** Test results @ steps:{} ***\\n'.format(global_step))\n",
    "    writer.write('*** Test results @ steps:{} ***\\n'.format(global_step))\n",
    "    for key, value in test_result.items():\n",
    "        logger.info('{} = {}\\n'.format(key, value))\n",
    "        writer.write('{} = {}\\n'.format(key, value))\n",
    "eval_results.update(test_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:15.226321Z",
     "end_time": "2023-05-02T01:26:15.663036Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 7: Save model**\n",
    "Reference: [Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/02/2023 01:26:15 - INFO - __main__ - Model state_dict:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dummy_param \t torch.Size([0])\n",
      "layers.0.attention.self_attention.query.weight \t torch.Size([128, 128])\n",
      "layers.0.attention.self_attention.query.bias \t torch.Size([128])\n",
      "layers.0.attention.self_attention.key.weight \t torch.Size([128, 128])\n",
      "layers.0.attention.self_attention.key.bias \t torch.Size([128])\n",
      "layers.0.attention.self_attention.value.weight \t torch.Size([128, 128])\n",
      "layers.0.attention.self_attention.value.bias \t torch.Size([128])\n",
      "layers.0.attention.self_output.dense.weight \t torch.Size([128, 128])\n",
      "layers.0.attention.self_output.dense.bias \t torch.Size([128])\n",
      "layers.0.attention.self_output.layer_norm.weight \t torch.Size([128])\n",
      "layers.0.attention.self_output.layer_norm.bias \t torch.Size([128])\n",
      "layers.1.attention.self_attention.query.weight \t torch.Size([128, 128])\n",
      "layers.1.attention.self_attention.query.bias \t torch.Size([128])\n",
      "layers.1.attention.self_attention.key.weight \t torch.Size([128, 128])\n",
      "layers.1.attention.self_attention.key.bias \t torch.Size([128])\n",
      "layers.1.attention.self_attention.value.weight \t torch.Size([128, 128])\n",
      "layers.1.attention.self_attention.value.bias \t torch.Size([128])\n",
      "layers.1.attention.self_output.dense.weight \t torch.Size([128, 128])\n",
      "layers.1.attention.self_output.dense.bias \t torch.Size([128])\n",
      "layers.1.attention.self_output.layer_norm.weight \t torch.Size([128])\n",
      "layers.1.attention.self_output.layer_norm.bias \t torch.Size([128])\n",
      "embeddings.dx_embeddings.weight \t torch.Size([3250, 128])\n",
      "embeddings.proc_embeddings.weight \t torch.Size([2211, 128])\n",
      "embeddings.visit_embeddings.weight \t torch.Size([1, 128])\n",
      "embeddings.layernorm.weight \t torch.Size([128])\n",
      "embeddings.layernorm.bias \t torch.Size([128])\n",
      "pooler.dense.weight \t torch.Size([128, 128])\n",
      "pooler.dense.bias \t torch.Size([128])\n",
      "classifier.weight \t torch.Size([2, 128])\n",
      "classifier.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# print model's state_dict\n",
    "logger.info('Model state_dict:')\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:15.650382Z",
     "end_time": "2023-05-02T01:26:15.663271Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# if enable save model option, save the model\n",
    "if args.save_model:\n",
    "    torch.save({\n",
    "        'epochs_trained': epochs_trained,\n",
    "        'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': training_outputs['loss'],\n",
    "        'all_hidden_states': training_outputs['all_hidden_states'],\n",
    "        'all_attentions': training_outputs['all_attentions']\n",
    "    }, args.output_dir + '/model.pt')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T01:26:15.650481Z",
     "end_time": "2023-05-02T01:26:15.718702Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

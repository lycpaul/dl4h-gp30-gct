{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:53.597397Z",
     "end_time": "2023-04-25T23:02:54.951776Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from gctpyhealth.process_eicu_dataset import get_eicu_datasets\n",
    "from gctpyhealth.utils import *\n",
    "from gctpyhealth.gct import GCT\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchsummary as summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:54.954463Z",
     "end_time": "2023-04-25T23:02:54.956543Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "\n",
    "args = Args()\n",
    "args.learning_rate = 0.00022\n",
    "args.max_steps = 10  ### for short run # 1000000\n",
    "args.do_train = True\n",
    "args.do_eval = True\n",
    "args.do_test = True\n",
    "args.warmup = 0.05  # default\n",
    "args.intermediate_size = 256  # default\n",
    "args.eps = 1e-8  # default\n",
    "args.max_grad_norm = 1.0  # default\n",
    "args.eval_batch_size = 32\n",
    "args.logging_steps = 100  # default\n",
    "args.num_train_epochs = 0  # default\n",
    "args.seed = 42  # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:56.595802Z",
     "end_time": "2023-04-25T23:02:56.603011Z"
    }
   },
   "outputs": [],
   "source": [
    "label_key = \"expired\"\n",
    "fold = 0\n",
    "data_dir = \"data\"\n",
    "output_dir = \"eicu_output/model_pyhealth\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:57.858500Z",
     "end_time": "2023-04-25T23:02:57.868798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the log data\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "logging_dir = os.path.join(output_dir, 'logging')\n",
    "if not os.path.exists(logging_dir):\n",
    "    os.makedirs(logging_dir)\n",
    "tb_writer = SummaryWriter(log_dir=logging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loading the eICU dataset\n",
    "from pyhealth.datasets import eICUDataset\n",
    "\n",
    "print('Loading eICU dataset')\n",
    "eicu_ds = eICUDataset(\n",
    "    root='../../eicu_csv',\n",
    "    tables=[\"admissionDx\", \"diagnosisString\", \"treatment\"],\n",
    "    refresh_cache=False,\n",
    "    dev=True\n",
    ")\n",
    "\n",
    "eicu_ds.stat()\n",
    "eicu_ds.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:03:06.047301Z",
     "end_time": "2023-04-25T23:03:09.446626Z"
    }
   },
   "outputs": [],
   "source": [
    "# fetch the datatset from caches\n",
    "datasets, prior_guides = get_eicu_datasets(data_dir, fold=fold)\n",
    "train_dataset, eval_dataset, test_dataset = datasets\n",
    "train_priors, eval_priors, test_priors = prior_guides\n",
    "train_priors_dataset = eICUPriorDataset(train_priors)\n",
    "eval_priors_dataset = eICUPriorDataset(eval_priors)\n",
    "test_priors_dataset = eICUPriorDataset(test_priors)\n",
    "\n",
    "# prepare data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train_priors_dataloader = DataLoader(train_priors_dataset,\n",
    "                                     batch_size=batch_size, collate_fn=priors_collate_fn)\n",
    "eval_priors_dataloader = DataLoader(eval_priors_dataset,\n",
    "                                    batch_size=batch_size, collate_fn=priors_collate_fn)\n",
    "test_priors_dataloader = DataLoader(test_priors_dataset,\n",
    "                                    batch_size=batch_size, collate_fn=priors_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check if cuda is available\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.set_device(device)\n",
    "    logger.info('***** Using CUDA device *****')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from pyhealth.models import Transformer\n",
    "from gctpyhealth.gct import GCT\n",
    "\n",
    "model = GCT(\n",
    "    dataset=eicu_ds,\n",
    "    feature_keys=['conditions_hash',\n",
    "                  'conditions_mask',\n",
    "                  'procedures_hash',\n",
    "                  'procedures_mask'],\n",
    "    label_key=\"label\",\n",
    "    mode=\"binary\",\n",
    ")\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare optimizer, scheduler\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "if args.max_steps > 0:\n",
    "    max_steps = args.max_steps\n",
    "    num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "        args.max_steps % num_update_steps_per_epoch > 0)\n",
    "else:\n",
    "    max_steps = int(num_update_steps_per_epoch * args.num_train_epochs)\n",
    "    num_train_epochs = args.num_train_epochs\n",
    "num_train_epochs = int(np.ceil(num_train_epochs))\n",
    "\n",
    "args.eval_steps = num_update_steps_per_epoch // 2\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=args.learning_rate)\n",
    "warmup_steps = max_steps // (1 / args.warmup)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "logger.info('***** Running Training *****')\n",
    "logger.info(' Num examples = {}'.format(len(train_dataloader.dataset)))\n",
    "logger.info(' Num epochs = {}'.format(num_train_epochs))\n",
    "logger.info(' Train batch size = {}'.format(batch_size))\n",
    "logger.info(' Total optimization steps = {}'.format(max_steps))\n",
    "\n",
    "epochs_trained = 0\n",
    "global_step = 0\n",
    "tr_loss = torch.tensor(0.0).to(device)\n",
    "logging_loss_scalar = 0.0\n",
    "model.zero_grad()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "train_pbar = trange(epochs_trained, num_train_epochs, desc='Epoch')\n",
    "for epoch in range(epochs_trained, num_train_epochs):\n",
    "    epoch_pbar = tqdm(train_dataloader, desc='Iteration')\n",
    "    for data, priors_data in zip(train_dataloader, train_priors_dataloader):\n",
    "        model.train()\n",
    "        data, priors_data = prepare_data(data, priors_data, device)\n",
    "\n",
    "        # [loss, logits, all_hidden_states, all_attentions]\n",
    "        outputs = model(data, priors_data)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.detach()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if (args.logging_steps > 0 and global_step % args.logging_steps == 0):\n",
    "            logs = {}\n",
    "            tr_loss_scalar = tr_loss.item()\n",
    "            logs['loss'] = (tr_loss_scalar - logging_loss_scalar) / args.logging_steps\n",
    "            logs['learning_rate'] = scheduler.get_last_lr()[0]\n",
    "            logging_loss_scalar = tr_loss_scalar\n",
    "            if tb_writer:\n",
    "                for k, v in logs.items():\n",
    "                    if isinstance(v, (int, float)):\n",
    "                        tb_writer.add_scalar(k, v, global_step)\n",
    "                tb_writer.flush()\n",
    "            output = {**logs, **{\"step\": global_step}}\n",
    "            print(output)\n",
    "\n",
    "        if (args.eval_steps > 0 and global_step % args.eval_steps == 0):\n",
    "            metrics = prediction_loop(device, label_key, model, eval_dataloader, eval_priors_dataloader)\n",
    "            logger.info('**** Checkpoint Eval Results ****')\n",
    "            for key, value in metrics.items():\n",
    "                logger.info('{} = {}'.format(key, value))\n",
    "                tb_writer.add_scalar(key, value, global_step)\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "    epoch_pbar.close()\n",
    "    train_pbar.update(1)\n",
    "    if global_step >= max_steps:\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "train_pbar.close()\n",
    "if tb_writer:\n",
    "    tb_writer.close()\n",
    "\n",
    "logging.info('\\n\\nTraining completed')\n",
    "\n",
    "eval_results = {}\n",
    "logger.info('*** Evaluate ***')\n",
    "logger.info(' Num examples = {}'.format(len(eval_dataloader.dataset)))\n",
    "eval_result = prediction_loop(device, label_key, model, eval_dataloader, eval_priors_dataloader)\n",
    "output_eval_file = os.path.join(output_dir, 'eval_results.txt')\n",
    "with open(output_eval_file, 'w') as writer:\n",
    "    logger.info('*** Eval Results ***')\n",
    "    for key, value in eval_result.items():\n",
    "        logger.info(\"{} = {}\".format(key, value))\n",
    "        writer.write('{} = {}'.format(key, value))\n",
    "eval_results.update(eval_result)\n",
    "\n",
    "logging.info('*** Test ***')\n",
    "# predict\n",
    "test_result = prediction_loop(device, label_key, model, test_dataloader, test_priors_dataloader, description='Testing')\n",
    "output_test_file = os.path.join(output_dir, 'test_results.txt')\n",
    "with open(output_test_file, 'w') as writer:\n",
    "    logger.info('**** Test results ****')\n",
    "    for key, value in test_result.items():\n",
    "        logger.info('{} = {}'.format(key, value))\n",
    "        writer.write('{} = {}'.format(key, value))\n",
    "eval_results.update(test_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The training runs without an error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyHealth Example Code\n",
    "\n",
    "If we implement all model code in PyHealth-compatible way, below codes should be run without an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyhealth_test = GraphConvolutionalTransformer(\n",
    "#         dataset=dataset,\n",
    "#         label_key=\"label\",\n",
    "#         mode=\"binary\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyhealth.datasets import get_dataloader\n",
    "# train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)\n",
    "# data_batch = next(iter(train_loader))\n",
    "#\n",
    "# ret = model(**data_batch)\n",
    "# print(ret) ## the output should be in this format\n",
    "# #{\n",
    "#    'loss': tensor(0.8872, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
    "#    'y_prob': tensor([[0.5008], [0.6614]], grad_fn=<SigmoidBackward0>),\n",
    "#    'y_true': tensor([[1.], [0.]]),\n",
    "#    'logit': tensor([[0.0033], [0.6695]], grad_fn=<AddmmBackward0>)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

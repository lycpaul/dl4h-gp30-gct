{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:53.597397Z",
     "end_time": "2023-04-25T23:02:54.951776Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from process_eicu_dataset import get_eicu_datasets\n",
    "from utils import *\n",
    "from gct_my import GraphConvolutionalTransformer\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchsummary as summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:54.954463Z",
     "end_time": "2023-04-25T23:02:54.956543Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.learning_rate = 0.00022\n",
    "args.max_steps=10 ### for short run # 1000000\n",
    "args.do_train = True\n",
    "args.do_eval = True\n",
    "args.do_test = True\n",
    "args.warmup = 0.05 # default\n",
    "args.intermediate_size = 256 # default\n",
    "args.eps = 1e-8 # default\n",
    "args.max_grad_norm = 1.0 # default\n",
    "args.eval_batch_size = 32  \n",
    "args.logging_steps=100 # default\n",
    "args.num_train_epochs = 0 # default\n",
    "args.seed = 42 # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:56.595802Z",
     "end_time": "2023-04-25T23:02:56.603011Z"
    }
   },
   "outputs": [],
   "source": [
    "label_key = \"readmission\"\n",
    "fold = 0\n",
    "data_dir = \"data\"\n",
    "output_dir=f'eicu_output/model_pyhealth'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:57.858500Z",
     "end_time": "2023-04-25T23:02:57.868798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the log data\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "logging_dir = os.path.join(output_dir, 'logging')\n",
    "if not os.path.exists(logging_dir):\n",
    "    os.makedirs(logging_dir)\n",
    "tb_writer = SummaryWriter(log_dir=logging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:02:58.937376Z",
     "end_time": "2023-04-25T23:02:58.946930Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyhealth.datasets import SampleEHRDataset\n",
    "\n",
    "samples = [\n",
    "        {\n",
    "            \"patient_id\": \"patient-0\",\n",
    "            \"visit_id\": \"visit-0\",\n",
    "            \"list_codes\": [\"505800458\", \"50580045810\", \"50580045811\"],  # NDC\n",
    "            \"list_vectors\": [[1.0, 2.55, 3.4], [4.1, 5.5, 6.0]],\n",
    "            \"list_list_codes\": [[\"A05B\", \"A05C\", \"A06A\"], [\"A11D\", \"A11E\"]],  # ATC-4\n",
    "            \"list_list_vectors\": [\n",
    "                [[1.8, 2.25, 3.41], [4.50, 5.9, 6.0]],\n",
    "                [[7.7, 8.5, 9.4]],\n",
    "            ],\n",
    "            \"label\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"patient_id\": \"patient-0\",\n",
    "            \"visit_id\": \"visit-1\",\n",
    "            \"list_codes\": [\n",
    "                \"55154191800\",\n",
    "                \"551541928\",\n",
    "                \"55154192800\",\n",
    "                \"705182798\",\n",
    "                \"70518279800\",\n",
    "            ],\n",
    "            \"list_vectors\": [[1.4, 3.2, 3.5], [4.1, 5.9, 1.7]],\n",
    "            \"list_list_codes\": [[\"A04A\", \"B035\", \"C129\"], [\"A07B\", \"A07C\"]],\n",
    "            \"list_list_vectors\": [\n",
    "                [[1.0, 2.8, 3.3], [4.9, 5.0, 6.6]],\n",
    "                [[7.7, 8.4, 1.3]],\n",
    "            ],\n",
    "            \"label\": 0,\n",
    "        },\n",
    "    ]\n",
    "dataset = SampleEHRDataset(samples=samples, dataset_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T23:03:06.047301Z",
     "end_time": "2023-04-25T23:03:09.446626Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets, prior_guides = get_eicu_datasets(data_dir, fold=fold)\n",
    "train_dataset, eval_dataset, test_dataset = datasets\n",
    "train_priors, eval_priors, test_priors = prior_guides\n",
    "train_priors_dataset = eICUDataset(train_priors)\n",
    "eval_priors_dataset = eICUDataset(eval_priors)\n",
    "test_priors_dataset = eICUDataset(test_priors)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train_priors_dataloader = DataLoader(train_priors_dataset, batch_size=batch_size, collate_fn=priors_collate_fn)\n",
    "eval_priors_dataloader = DataLoader(eval_priors_dataset, batch_size=batch_size, collate_fn=priors_collate_fn)\n",
    "test_priors_dataloader = DataLoader(test_priors_dataset, batch_size=batch_size, collate_fn=priors_collate_fn)\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.set_device(device)\n",
    "    logger.info('***** Using CUDA device *****')\n",
    "\n",
    "model = GraphConvolutionalTransformer(dataset)\n",
    "model = model.to(device)\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "if args.max_steps > 0:\n",
    "    max_steps = args.max_steps\n",
    "    num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "        args.max_steps % num_update_steps_per_epoch > 0)\n",
    "else:\n",
    "    max_steps = int(num_update_steps_per_epoch * args.num_train_epochs)\n",
    "    num_train_epochs = args.num_train_epochs\n",
    "num_train_epochs = int(np.ceil(num_train_epochs))\n",
    "\n",
    "args.eval_steps = num_update_steps_per_epoch // 2\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=args.learning_rate)\n",
    "warmup_steps = max_steps // (1 / args.warmup)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "logger.info('***** Running Training *****')\n",
    "logger.info(' Num examples = {}'.format(len(train_dataloader.dataset)))\n",
    "logger.info(' Num epochs = {}'.format(num_train_epochs))\n",
    "logger.info(' Train batch size = {}'.format(batch_size))\n",
    "logger.info(' Total optimization steps = {}'.format(max_steps))\n",
    "\n",
    "epochs_trained = 0\n",
    "global_step = 0\n",
    "tr_loss = torch.tensor(0.0).to(device)\n",
    "logging_loss_scalar = 0.0\n",
    "model.zero_grad()\n",
    "\n",
    "train_pbar = trange(epochs_trained, num_train_epochs, desc='Epoch')\n",
    "for epoch in range(epochs_trained, num_train_epochs):\n",
    "    epoch_pbar = tqdm(train_dataloader, desc='Iteration')\n",
    "    for data, priors_data in zip(train_dataloader, train_priors_dataloader):\n",
    "        model.train()\n",
    "        data, priors_data = prepare_data(data, priors_data, device)\n",
    "\n",
    "        # [loss, logits, all_hidden_states, all_attentions]\n",
    "        outputs = model(data, priors_data)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.detach()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if (args.logging_steps > 0 and global_step % args.logging_steps == 0):\n",
    "            logs = {}\n",
    "            tr_loss_scalar = tr_loss.item()\n",
    "            logs['loss'] = (tr_loss_scalar - logging_loss_scalar) / args.logging_steps\n",
    "            logs['learning_rate'] = scheduler.get_last_lr()[0]\n",
    "            logging_loss_scalar = tr_loss_scalar\n",
    "            if tb_writer:\n",
    "                for k, v in logs.items():\n",
    "                    if isinstance(v, (int, float)):\n",
    "                        tb_writer.add_scalar(k, v, global_step)\n",
    "                tb_writer.flush()\n",
    "            output = {**logs, **{\"step\": global_step}}\n",
    "            print(output)\n",
    "\n",
    "        if (args.eval_steps > 0 and global_step % args.eval_steps == 0):\n",
    "            metrics = prediction_loop(device, label_key, model, eval_dataloader, eval_priors_dataloader)\n",
    "            logger.info('**** Checkpoint Eval Results ****')\n",
    "            for key, value in metrics.items():\n",
    "                logger.info('{} = {}'.format(key, value))\n",
    "                tb_writer.add_scalar(key, value, global_step)\n",
    "\n",
    "        epoch_pbar.update(1)\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "    epoch_pbar.close()\n",
    "    train_pbar.update(1)\n",
    "    if global_step >= max_steps:\n",
    "        break\n",
    "\n",
    "train_pbar.close()\n",
    "if tb_writer:\n",
    "    tb_writer.close()\n",
    "\n",
    "logging.info('\\n\\nTraining completed')\n",
    "\n",
    "eval_results = {}\n",
    "logger.info('*** Evaluate ***')\n",
    "logger.info(' Num examples = {}'.format(len(eval_dataloader.dataset)))\n",
    "eval_result = prediction_loop(device, label_key, model, eval_dataloader, eval_priors_dataloader)\n",
    "output_eval_file = os.path.join(output_dir, 'eval_results.txt')\n",
    "with open(output_eval_file, 'w') as writer:\n",
    "    logger.info('*** Eval Results ***')\n",
    "    for key, value in eval_result.items():\n",
    "        logger.info(\"{} = {}\".format(key, value))\n",
    "        writer.write('{} = {}'.format(key, value))\n",
    "eval_results.update(eval_result)\n",
    "\n",
    "logging.info('*** Test ***')\n",
    "# predict\n",
    "test_result = prediction_loop(device, label_key, model, test_dataloader, test_priors_dataloader, description='Testing')\n",
    "output_test_file = os.path.join(output_dir, 'test_results.txt')\n",
    "with open(output_test_file, 'w') as writer:\n",
    "    logger.info('**** Test results ****')\n",
    "    for key, value in test_result.items():\n",
    "        logger.info('{} = {}'.format(key, value))\n",
    "        writer.write('{} = {}'.format(key, value))\n",
    "eval_results.update(test_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The training runs without an error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyHealth Example Code\n",
    "\n",
    "If we implement all model code in PyHealth-compatible way, below codes should be run without an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhealth_test = GraphConvolutionalTransformer(\n",
    "        dataset=dataset,\n",
    "        label_key=\"label\",\n",
    "        mode=\"binary\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import get_dataloader\n",
    "train_loader = get_dataloader(dataset, batch_size=2, shuffle=True)\n",
    "data_batch = next(iter(train_loader))\n",
    "\n",
    "ret = model(**data_batch)\n",
    "print(ret) ## the output should be in this format\n",
    "#{\n",
    "#    'loss': tensor(0.8872, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
    "#    'y_prob': tensor([[0.5008], [0.6614]], grad_fn=<SigmoidBackward0>),\n",
    "#    'y_true': tensor([[1.], [0.]]),\n",
    "#    'logit': tensor([[0.0033], [0.6695]], grad_fn=<AddmmBackward0>)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
